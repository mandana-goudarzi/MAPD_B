{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3111693a-64c6-416e-a811-bd3b2396b6e0",
   "metadata": {},
   "source": [
    "### Benchmark file\n",
    "- In this file I removed the heatmap generation part since it wasn't meant for efficiency but rather than visuals. Here I am getting the execution time of the whole calculation until the filtered dataframe count. I also removed the persist part because the calculations are not so heavy.\n",
    "- I will add the persist to differentiate between embedding creation and cosine similarity df creation.\n",
    "\n",
    "The combinations of configurations I am using which I added in the whole_time.csv file (for the whole runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b87ec7-2990-4bb8-a637-1c5c6f7e3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPython library for collecting the sequences from cif files\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cf507f-7791-4b9b-899b-0433a31fa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import os\n",
    "import io\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Pyspark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700fe0d1-c9ef-41b3-b363-961b9fe463b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/08 22:11:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Proteindata spark application\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .config(\"spark.driver.host\", \"10.67.22.219\") \\\n",
    "    .config(\"spark.driver.port\",\"6066\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.executor.cores\",\"2\")\\\n",
    "    .config(\"spark.sql.optimizer.enableRangeJoin\", \"true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ca5aad-3e7d-4e5d-9fb4-85aaa3745e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.config(\"spark.memoryOverheadFactor\", \"0.05\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1c570a-199d-4c57-b6d7-2173f3c763aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a6027b-bcf8-431b-9686-d091bd82601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "start_time0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a6ea2c-43a1-4fcd-be8c-5cc26a6df9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the file paths\n",
    "base_path = '/data_files/pdb_files'\n",
    "base_path_edit = '/data_files/pdb_files/{}'\n",
    "file_names = os.listdir(base_path)\n",
    "# The size of our original folder is about 2GB consisting of 1712 cif files\n",
    "# we needed to reduce the data size in order to be able to compute without crashing\n",
    "# we will examine the reasons of the crashing in the benchmarking\n",
    "file_list = [base_path_edit.format(i) for i in file_names][-300:]\n",
    "files_rdd = sc.parallelize(file_list)#numSlices=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c16b181-ef99-4972-8f03-188894c60d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting all of the paths to see if there are any errors\n",
    "files_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ea865f-353a-4265-86ec-233b9c6c2aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files_rdd.coalesce(12)\n",
    "files_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1a7fe6-05a7-487c-a79c-0892a18fd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file):\n",
    "\n",
    "    cif_parser = MMCIFParser(QUIET=True) # CIF file parser\n",
    "    length = 0 # Setting the length initially to 0 for error correction\n",
    "    name = file.split('/')[3].split('.')[0] # Getting the id of the protein\n",
    "    structure = cif_parser.get_structure(\"protein\", file) # getting structure ? try \"protein\"\n",
    "\n",
    "    # Dictionary for residue names\n",
    "    d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "    'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    "    'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    "    'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "    # Creating the sequence   \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            sequence = [d3to1.get(residue.get_resname(), 'X') for residue in chain.get_residues()]\n",
    "            length = len(sequence)\n",
    "    \n",
    "    return name,sequence,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d984795-af10-4d5e-ba73-a4859a88a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating an RDD for the tokens\n",
    "def tokens_df_creator(file_path):\n",
    "\n",
    "    data = []\n",
    "    name, sequence, length = parse_file(file_path)\n",
    "    row_value = {\n",
    "        'id':name,\n",
    "        'length':length,\n",
    "        'tokens':sequence, # sequence is tokenized like [\"A\",\"B\",\"K\",...] (each residue is a token)\n",
    "    }\n",
    "    data.append(Row(**row_value)) # Creating rows containing id, length and the sequence of each file in the folder\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Turning the RDD into a DF for easier usage\n",
    "tokens_rdd = files_rdd.flatMap(tokens_df_creator) # FlatMap applied to the RDD\n",
    "tokens_df = tokens_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8911f26-292c-401d-bfdf-b24db40f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary from the .vec file\n",
    "# .vec file contains the keys (residues) and their corresponding embedding with size 1024\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        letter_token = line.rstrip().split()\n",
    "        # For each residue creating a DenseVector containing the embedding obtained from the .vec file\n",
    "        data[letter_token[0]] = DenseVector([float(letter) for letter in letter_token[1:]]) \n",
    "    return data\n",
    "\n",
    "vec_dict = load_vectors('/data_files/prot_bert.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4f32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the dictionary\n",
    "vec_broadcast = sc.broadcast(vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a58f139-a70e-4f4a-980c-45f8ed2da280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the sequence\n",
    "# The embedding is done using a UDF\n",
    "@F.udf(ArrayType(VectorUDT()))\n",
    "def embed_sequence(tokens_list):\n",
    "    return [vec_broadcast.value[token] for token in tokens_list if token in vec_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fa4d1e-bcec-4fbd-b7b4-f1b1869f8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings are added as a new column (UDF applied to the DataFrame)\n",
    "tokens_df = tokens_df.withColumn(\"embeddings\",embed_sequence(tokens_df.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a6634a-e0de-4cb2-9963-4499b602d05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating another udf to get the mean of each embedding row\n",
    "@F.udf(VectorUDT())\n",
    "def mean_calculator(embedding,length):\n",
    "    mean_embedding = sum(embedding)/length # getting the mean of the embeddings in axis=0\n",
    "    return mean_embedding   \n",
    "\n",
    "# Created mean embedding is added as a new column\n",
    "tokens_df = tokens_df.withColumn(\"mean_embed\",mean_calculator(tokens_df.embeddings,tokens_df.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c9cc88-3b24-4e26-9c56-ca0f8077bff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting a subset of the tokens_df\n",
    "mean_embed_df = tokens_df.select(\"id\",\"mean_embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109c1bb-4bf3-411f-9254-5f07332c180a",
   "metadata": {},
   "source": [
    "##### For calculating how long each complete task take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfefd414-79cb-4331-ad9f-01d1c1917dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time0 = time.time()\n",
    "execution_time0 = end_time0 - start_time0\n",
    "mean_embed_df = mean_embed_df.persist()\n",
    "\n",
    "start_time1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92f7e3d-b478-4921-9b08-c5343107f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2 views from the same mean_embed_df\n",
    "mean_embed_df.withColumnRenamed('id', 'id1').withColumnRenamed('mean_embed', 'embed_1').createOrReplaceTempView(\"df1\")\n",
    "\n",
    "mean_embed_df.withColumnRenamed('id', 'id2').withColumnRenamed('mean_embed', 'embed_2').createOrReplaceTempView(\"df2\")\n",
    "\n",
    "# Using sql to join the dfs using a condition which reduces the number of unnecessary calculation\n",
    "# caused by the joining 2 dfs to create all the possible pairs\n",
    "joined_df = spark.sql(\n",
    "\"\"\"SELECT *\n",
    "FROM df1, df2\n",
    "WHERE df1.id1 < df2.id2\"\"\") # using this condition we are creating only the upper triangle of the expected cross joined \"matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20cd727d-ad48-4c0b-bc40-1d6d0d6d7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DenseVector to an array\n",
    "joined_df = joined_df.withColumn(\"embed_1\", vector_to_array(F.col(\"embed_1\")))\n",
    "joined_df = joined_df.withColumn(\"embed_2\", vector_to_array(F.col(\"embed_2\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2dc0ab3-7ce9-4020-8a72-70ce9f5f6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dot product, norms, and cosine similarity directly\n",
    "df_cos_sim = (\n",
    "    joined_df.withColumn(\"dot_prod\", F.expr(\"aggregate(zip_with(embed_1, embed_2, (x, y) -> x * y), 0D, (acc, v) -> acc + v)\"))\n",
    "      .withColumn(\"norm_1\", F.expr(\"sqrt(aggregate(embed_1, 0D, (acc, v) -> acc + v * v))\"))\n",
    "      .withColumn(\"norm_2\", F.expr(\"sqrt(aggregate(embed_2, 0D, (acc, v) -> acc + v * v))\"))\n",
    "      .withColumn(\"cosine_similarity\", F.col(\"dot_prod\") / (F.col(\"norm_1\") * F.col(\"norm_2\")))\n",
    "      .drop(\"dot_prod\", \"norm_1\", \"norm_2\",\"embed_1\",\"embed_2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2875727e-da0f-44de-b1ba-a45f89e085f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the data since being really close to 1 is not surprising\n",
    "filtered_df = df_cos_sim.filter(\"cosine_similarity < 0.955\")\n",
    "filtered_df.count() # We are expecting a small amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9557a11e-32e3-45a5-b6cb-aa6578cdb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end_time = time.time()\n",
    "#execution_time = end_time - start_time\n",
    "end_time1 = time.time()\n",
    "execution_time1 = end_time1 - start_time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78e23882-9753-4292-aa6a-563d198fe701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.365358114242554"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_time0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ba7deb-7136-412e-abfa-e087a1383ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.8496322631836"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_time1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c3767-daa0-4581-9e6b-c5805cc7fdfa",
   "metadata": {},
   "source": [
    "### Stopping the context and the spark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8396bcc2-5877-40c0-80ba-608205c04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bd7678b-5720-4d3b-8e15-9e1385d7263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
